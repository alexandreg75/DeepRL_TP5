## Exercice 1 ‚Äî Exploration de Gymnasium

### Visualisation de l‚Äôagent al√©atoire

![Agent al√©atoire sur LunarLander](./random_agent.gif)

---

### Espaces de l‚Äôenvironnement

Espace d‚Äôobservation (capteurs) :
Box(..., shape=(8,), float32)

L‚Äôenvironnement fournit donc un vecteur de **8 variables continues** :
- position horizontale
- position verticale
- vitesse horizontale
- vitesse verticale
- angle du module
- vitesse angulaire
- contact jambe gauche
- contact jambe droite

Espace d‚Äôaction (moteurs) :
Discrete(4)

Les actions possibles sont :
- 0 : Ne rien faire
- 1 : Moteur lat√©ral gauche
- 2 : Moteur principal
- 3 : Moteur lat√©ral droit

---

### Rapport de vol

```text
--- RAPPORT DE VOL ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -128.62 points
Allumages moteur principal : 23
Allumages moteurs lat√©raux : 64
Dur√©e du vol : 116 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'random_agent.gif'


## Exercice 2 ‚Äî Entra√Ænement et √©valuation PPO (Stable Baselines3)

### √âvolution de `ep_rew_mean` pendant l‚Äôentra√Ænement

Pendant l‚Äôapprentissage PPO, la m√©trique `ep_rew_mean` (r√©compense moyenne par √©pisode) a nettement augment√©.

- Au d√©but (vers `total_timesteps = 2048`), on observe `ep_rew_mean ‚âà -194`.
- √Ä la fin de l‚Äôentra√Ænement (vers `total_timesteps ‚âà 500000`), `ep_rew_mean` est mont√© autour de **~100** (ex: `ep_rew_mean ‚âà 102`).

Cette hausse indique que l‚Äôagent apprend progressivement une strat√©gie de pilotage plus efficace qu‚Äôun agent al√©atoire, m√™me si la performance finale n‚Äôest pas suffisante pour ‚Äúr√©soudre‚Äù l‚Äôenvironnement.

---

### Visualisation de l‚Äôagent PPO entra√Æn√©

![Agent PPO entra√Æn√© sur LunarLander](./trained_ppo_agent.gif)

---

### Rapport de vol PPO

```text
--- RAPPORT DE VOL PPO ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -11.21 points
Allumages moteur principal : 147
Allumages moteurs lat√©raux : 138
Dur√©e du vol : 285 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'trained_ppo_agent.gif'


## Exercice 3 ‚Äî Reward Engineering (Wrappers et Hacking)

### 3.a ‚Äî Wrapper de p√©nalit√© carburant

On a cr√©√© un wrapper Gymnasium qui intercepte `step(action)` et modifie la r√©compense si l‚Äôagent utilise le moteur principal (action `2`).  
R√©compense modifi√©e :
\[
r'(s,a,s') = r(s,a,s') - 50\cdot\mathbb{1}[a=2]
\]

### 3.b ‚Äî Ex√©cution, t√©l√©m√©trie et strat√©gie observ√©e

#### Sortie terminal

```text
python3 reward_hacker.py 
--- ENTRA√éNEMENT DE L'AGENT RADIN ---
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
...
Entra√Ænement termin√©.

--- √âVALUATION ET T√âL√âM√âTRIE ---

--- RAPPORT DE VOL PPO HACKED ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -104.40 points
Allumages moteur principal : 0
Allumages moteurs lat√©raux : 10
Dur√©e du vol : 79 frames
Vid√©o du nouvel agent sauvegard√©e sous 'hacked_agent.gif'


### Interpr√©tation en termes de MDP

En modifiant la r√©compense via le wrapper, nous avons implicitement modifi√© la fonction de r√©compense du MDP sous-jacent.  
L‚Äôagent n‚Äôinteragit plus avec l‚Äôenvironnement original \( (S, A, P, R, \gamma) \), mais avec un nouveau MDP \( (S, A, P, R', \gamma) \).

Le comportement appris est donc parfaitement coh√©rent avec le nouvel objectif optimis√©.  
Le probl√®me ne vient pas de l‚Äôalgorithme PPO, mais de la mauvaise sp√©cification de la fonction de r√©compense.

Ce ph√©nom√®ne illustre un principe central en Reinforcement Learning :
"Un agent optimise exactement ce qu‚Äôon lui demande, pas ce qu‚Äôon voulait dire."

## Exercice 4

python3 ood_agent.py
--- √âVALUATION OOD : GRAVIT√â FAIBLE ---

--- RAPPORT DE VOL PPO (GRAVIT√â MODIFI√âE) ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -53.58 points
Allumages moteur principal : 25
Allumages moteurs lat√©raux : 191
Dur√©e du vol : 220 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'ood_agent.gif'

### Observation d√©taill√©e du GIF

Dans la gravit√© faible, le vaisseau adopte une trajectoire diagonale vers la droite et finit par atterrir (ou s‚Äô√©craser) tr√®s loin de la zone centrale, du c√¥t√© droit du terrain.

On observe :

- Une d√©rive lat√©rale continue vers la droite
- De tr√®s nombreuses corrections lat√©rales (191 activations)
- Une stabilisation verticale relativement lente (gravit√© faible)
- Une absence de recentrage vers la zone d‚Äôatterrissage centrale

L‚Äôagent ne corrige jamais r√©ellement sa position horizontale vers le centre. Il semble appliquer une strat√©gie "pr√©-apprise" qui ne correspond plus √† la nouvelle dynamique.

### Pourquoi cette d√©rive diagonale appara√Æt-elle ?

√Ä l'entra√Ænement (gravit√© = -10.0), la chute est rapide.  
L‚Äôagent a appris que :

- La gravit√© corrige naturellement certaines d√©rives horizontales.
- Les corrections lat√©rales doivent √™tre courtes et pr√©cises.
- Le timing des moteurs d√©pend fortement de la vitesse verticale.

En gravit√© faible (-2.0), plusieurs effets changent :

1. La chute est beaucoup plus lente ‚Üí le vaisseau reste longtemps en altitude.
2. Les corrections lat√©rales ont un effet plus durable (moins de "freinage naturel" par la descente rapide).
3. Les petites erreurs horizontales s‚Äôaccumulent au lieu d‚Äô√™tre amorties.

Ainsi, une l√©g√®re inclinaison initiale produit une acc√©l√©ration horizontale prolong√©e, ce qui explique la d√©rive diagonale continue vers la droite.

L‚Äôagent applique une politique œÄ(a|s) optimis√©e pour une dynamique o√π :
\[
P_{\text{train}}(s_{t+1} \mid s_t, a_t)
\]
√©tait d√©finie avec g = -10.

En test, la transition devient :
\[
P_{\text{test}} \neq P_{\text{train}}
\]

Donc les actions produisent des effets non anticip√©s par le r√©seau.  
La politique n‚Äôest plus calibr√©e pour cette physique, ce qui provoque une instabilit√© et une d√©rive syst√©matique.

Ce ph√©nom√®ne illustre parfaitement un probl√®me de g√©n√©ralisation Out-of-Distribution :  
le mod√®le n‚Äôa pas appris les lois physiques abstraites du syst√®me, mais une strat√©gie sp√©cifique adapt√©e √† la distribution d‚Äôentra√Ænement.

Un simple changement de param√®tre physique suffit √† r√©v√©ler la fragilit√© de la politique apprise.


Exercice 5 ‚Äî Bilan Ing√©nieur : Sim-to-Real

5.a ‚Äî Rendre l‚Äôagent robuste sans entra√Æner un mod√®le par lune

L‚Äô√©chec en gravit√© modifi√©e montre que l‚Äôagent a surappris la physique de l‚Äôenvironnement d‚Äôentra√Ænement. Pour √©viter d‚Äôentra√Æner un mod√®le sp√©cifique pour chaque lune, on peut mettre en place les strat√©gies suivantes :

1) Domain Randomization

Au lieu d‚Äôentra√Æner avec une gravit√© fixe, on entra√Æne l‚Äôagent sur une vari√©t√© de conditions physiques :

Gravit√© al√©atoire √† chaque √©pisode

Vent et turbulence variables

√âventuellement variations de masse ou de puissance moteur

Ainsi, l‚Äôagent apprend une strat√©gie robuste valable dans plusieurs environnements, et non adapt√©e √† un seul cas pr√©cis.

2) Ajouter les param√®tres physiques dans l‚Äôobservation

On peut fournir √† l‚Äôagent des informations explicites sur la gravit√© ou le vent dans son vecteur d‚Äô√©tat.
Il apprend alors √† adapter son comportement en fonction des conditions physiques rencontr√©es.
Un seul mod√®le peut ainsi fonctionner sur diff√©rentes lunes sans r√©-entra√Ænement complet.

Conclusion

Le Sim-to-Real Gap appara√Æt lorsque la dynamique r√©elle diff√®re de celle utilis√©e en simulation.
En diversifiant les conditions d‚Äôentra√Ænement et en rendant les param√®tres physiques observables, on am√©liore la robustesse du mod√®le sans changer d‚Äôalgorithme ni multiplier les mod√®les.