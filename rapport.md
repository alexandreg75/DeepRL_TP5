## Exercice 1 ‚Äî Exploration de Gymnasium

### Visualisation de l‚Äôagent al√©atoire

![Agent al√©atoire sur LunarLander](./random_agent.gif)

---

### Espaces de l‚Äôenvironnement

Espace d‚Äôobservation (capteurs) :  
Box(..., shape=(8,), float32)

L‚Äôenvironnement fournit donc un vecteur de **8 variables continues** :
- position horizontale  
- position verticale  
- vitesse horizontale  
- vitesse verticale  
- angle du module  
- vitesse angulaire  
- contact jambe gauche  
- contact jambe droite  

Espace d‚Äôaction (moteurs) :  
Discrete(4)

Les actions possibles sont :
- 0 : Ne rien faire  
- 1 : Moteur lat√©ral gauche  
- 2 : Moteur principal  
- 3 : Moteur lat√©ral droit  

---

### Rapport de vol

```text
--- RAPPORT DE VOL ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -128.62 points
Allumages moteur principal : 23
Allumages moteurs lat√©raux : 64
Dur√©e du vol : 116 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'random_agent.gif'
```

---

## Exercice 2 ‚Äî Entra√Ænement et √©valuation PPO (Stable Baselines3)

### √âvolution de `ep_rew_mean` pendant l‚Äôentra√Ænement

Pendant l‚Äôapprentissage PPO, la m√©trique `ep_rew_mean` (r√©compense moyenne par √©pisode) a nettement augment√©.

- Au d√©but (vers `total_timesteps = 2048`), on observe `ep_rew_mean ‚âà -194`.
- √Ä la fin de l‚Äôentra√Ænement (vers `total_timesteps ‚âà 500000`), `ep_rew_mean` est mont√© autour de **~100** (ex: `ep_rew_mean ‚âà 102`).

Cette hausse indique que l‚Äôagent apprend progressivement une strat√©gie de pilotage plus efficace qu‚Äôun agent al√©atoire, m√™me si la performance finale n‚Äôest pas suffisante pour ‚Äúr√©soudre‚Äù l‚Äôenvironnement.

---

### Visualisation de l‚Äôagent PPO entra√Æn√©

![Agent PPO entra√Æn√© sur LunarLander](./trained_ppo_agent.gif)

---

### Rapport de vol PPO

```text
--- RAPPORT DE VOL PPO ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -11.21 points
Allumages moteur principal : 147
Allumages moteurs lat√©raux : 138
Dur√©e du vol : 285 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'trained_ppo_agent.gif'
```

---

## Exercice 3 ‚Äî Reward Engineering (Wrappers et Hacking)

### 3.a ‚Äî Wrapper de p√©nalit√© carburant

On a cr√©√© un wrapper Gymnasium qui intercepte `step(action)` et modifie la r√©compense si l‚Äôagent utilise le moteur principal (action `2`).

R√©compense modifi√©e :

```text
r' = r - 50 si action == 2
```

### 3.b ‚Äî Ex√©cution, t√©l√©m√©trie et strat√©gie observ√©e

#### Sortie terminal

```text
python3 reward_hacker.py 
--- ENTRA√éNEMENT DE L'AGENT RADIN ---
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
...
Entra√Ænement termin√©.

--- √âVALUATION ET T√âL√âM√âTRIE ---

--- RAPPORT DE VOL PPO HACKED ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -104.40 points
Allumages moteur principal : 0
Allumages moteurs lat√©raux : 10
Dur√©e du vol : 79 frames
Vid√©o du nouvel agent sauvegard√©e sous 'hacked_agent.gif'
```

### Interpr√©tation en termes de MDP

En modifiant la r√©compense via le wrapper, nous avons implicitement modifi√© la fonction de r√©compense du MDP sous-jacent.  
L‚Äôagent n‚Äôinteragit plus avec l‚Äôenvironnement original, mais avec un nouveau MDP dont la r√©compense p√©nalise fortement l‚Äôaction 2.

Le comportement appris est donc parfaitement coh√©rent avec le nouvel objectif optimis√©.  
Le probl√®me ne vient pas de l‚Äôalgorithme PPO, mais de la mauvaise sp√©cification de la fonction de r√©compense.

Ce ph√©nom√®ne illustre un principe central en Reinforcement Learning :

> Un agent optimise exactement ce qu‚Äôon lui demande, pas ce qu‚Äôon voulait dire.

---

## Exercice 4 ‚Äî G√©n√©ralisation OOD

### Sortie terminal

```text
python3 ood_agent.py
--- √âVALUATION OOD : GRAVIT√â FAIBLE ---

--- RAPPORT DE VOL PPO (GRAVIT√â MODIFI√âE) ---
Issue du vol : CRASH D√âTECT√â üí•
R√©compense totale cumul√©e : -53.58 points
Allumages moteur principal : 25
Allumages moteurs lat√©raux : 191
Dur√©e du vol : 220 frames
Vid√©o de la t√©l√©m√©trie sauvegard√©e sous 'ood_agent.gif'
```

### Observation du GIF

Dans la gravit√© faible, le vaisseau adopte une trajectoire diagonale vers la droite et finit par s‚Äô√©craser tr√®s loin de la zone centrale.

On observe :

- Une d√©rive lat√©rale continue vers la droite  
- De tr√®s nombreuses corrections lat√©rales (191 activations)  
- Une stabilisation verticale lente  
- Une absence de recentrage vers la zone d‚Äôatterrissage centrale  

L‚Äôagent applique une strat√©gie adapt√©e √† la gravit√© d‚Äôentra√Ænement (-10.0), mais inadapt√©e √† la nouvelle dynamique (-2.0).

Ce ph√©nom√®ne illustre un probl√®me de g√©n√©ralisation Out-of-Distribution :  
la politique apprise est sp√©cialis√©e pour la distribution d‚Äôentra√Ænement et ne s‚Äôadapte pas aux nouvelles conditions physiques.

---

## Exercice 5 ‚Äî Bilan Ing√©nieur : Sim-to-Real

### 5.a ‚Äî Rendre l‚Äôagent robuste sans entra√Æner un mod√®le par lune

L‚Äô√©chec en gravit√© modifi√©e montre que l‚Äôagent a surappris la physique d‚Äôentra√Ænement.

Deux strat√©gies pour am√©liorer la robustesse :

**1) Domain Randomization**

Entra√Æner l‚Äôagent avec :
- Gravit√© al√©atoire √† chaque √©pisode  
- Vent et turbulence variables  
- Param√®tres physiques l√©g√®rement perturb√©s  

L‚Äôagent apprend ainsi une strat√©gie robuste valable sur plusieurs environnements.

**2) Ajouter les param√®tres physiques dans l‚Äôobservation**

Inclure la gravit√© ou le vent dans le vecteur d‚Äô√©tat.  
L‚Äôagent peut alors adapter son comportement aux conditions rencontr√©es.

### Conclusion

Le Sim-to-Real Gap appara√Æt lorsque la dynamique r√©elle diff√®re de celle utilis√©e en simulation.  
En diversifiant les conditions d‚Äôentra√Ænement et en rendant les param√®tres physiques observables, on am√©liore la robustesse du mod√®le sans changer d‚Äôalgorithme ni multiplier les mod√®les.